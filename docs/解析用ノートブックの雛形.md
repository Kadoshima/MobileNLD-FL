以下は、そのままJupyterに貼って動かせる「最高品質の解析ノートブック雛形」です。実験手順書と要件定義書の仕様に完全準拠し、欠測やフィールド有無の違いに頑健な実装にしてあります。各セルは上から順に実行してください。

セル1: 概要・環境チェック
```
# 解析ノートブック（雛形）
# プロジェクト: Context-Uncertainty–Driven Adaptive BLE Advertising for Ultra-Low-Power Wearable HAR
# 目的:
# - 実験データ（PPK2, Androidロガー, UART, meta）から主要指標を一貫算出
# - 図表テンプレ（ComEX用）を自動生成
# - 条件間の統計比較（対応ありt検定・効果量）を実施
# - 欠測やフィールド差異に頑健な前処理

import sys, os, json, math, glob, textwrap, datetime as dt, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import f1_score, classification_report

warnings.simplefilter("ignore")
pd.set_option("display.max_columns", 200)
print("Python:", sys.version)
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("scipy:", stats.__version__)
print("seaborn:", sns.__version__)
```

セル2: 設定（入出力パス、条件名の正規化、図保存設定）
```
# 入出力のルート
DATA_ROOT = "data/raw"         # 実験手順書の構成に合わせる
OUT_ROOT  = "data/results"     # 結果CSVなど
FIG_ROOT  = "figs"             # 図出力

os.makedirs(OUT_ROOT, exist_ok=True)
os.makedirs(FIG_ROOT, exist_ok=True)

# 条件の正規化（フォルダ名やmeta condition→表示名）
CONDITION_MAP = {
    "Baseline-F100": "Fixed-100ms",
    "Baseline-F200": "Fixed-200ms",
    "Baseline-F500": "Fixed-500ms",
    "Proposed-ADP":  "Adaptive",
    "Proposed-ADP-HighHyst": "Adaptive-HighHyst"
}

# 評価の主ペア（統計比較に使用）
PAIR_BASE = "Fixed-100ms"
PAIR_PROP = "Adaptive"

# 乱数シード（再現性）
np.random.seed(42)

# 図のスタイル
sns.set(style="whitegrid", context="talk")
plt.rcParams["figure.figsize"] = (10, 5)
plt.rcParams["savefig.dpi"] = 200
```

セル3: ユーティリティ（PPK2/Phone/UART/Metaの汎用ローダ）
```
def load_ppk2_csv(path: str) -> pd.DataFrame:
    """
    PPK2 CSVローダ。代表的なヘッダ名に対応し、標準列に正規化する。
    出力: t_s, i_mA, v_V
    """
    df = pd.read_csv(path)
    cols = {c.lower(): c for c in df.columns}
    # 典型: "Time (s)","Current (mA)","Voltage (V)"
    t_col = next((cols[k] for k in cols if "time" in k and "(s" in k), None)
    i_col = next((cols[k] for k in cols if "current" in k and "(ma" in k), None)
    v_col = next((cols[k] for k in cols if "voltage" in k), None)
    if t_col is None or i_col is None:
        raise ValueError(f"PPK2 CSV format unknown: {path}")
    out = pd.DataFrame({
        "t_s": df[t_col].astype(float),
        "i_mA": df[i_col].astype(float),
        "v_V": df[v_col].astype(float) if v_col else 3.0
    })
    return out

def parse_mfg_payload_hex(hexstr: str) -> dict:
    """
    Manufacturer DataのHex文字列からフィールドを抽出。
    仕様（推奨例。存在しない場合は可能な範囲で推定）:
      CompanyID(2B LE), Ver(1B), SessionID(2B),
      State(1B), Uq(1B 0-255), Battery(1B),
      Seq(1B), TickLSB_10ms(2B LE),
      Flags(1B) [bit0: SYNC中, bit1: STATE_CHANGE],
      [Optional] StateSeq(1B), StateChgTickLSB_10ms(2B)
    """
    try:
        b = bytes.fromhex(hexstr)
    except Exception:
        return {}
    d = {}
    def get_le16(off):
        return b[off] | (b[off+1] << 8)
    try:
        d["company_id"] = get_le16(0)
        d["ver"] = b[2]
        d["session_id"] = get_le16(3)
        d["state"] = b[5]          # 0=Quiet,1=Active,2=Uncertain
        d["u_q"] = b[6]            # 0-255
        d["battery_pct"] = b[7]
        d["seq"] = b[8]
        d["tick_lsb_10ms"] = get_le16(9)
        d["flags"] = b[11] if len(b) > 11 else 0
        # optional fields
        if len(b) >= 13:
            d["state_seq"] = b[12]
        if len(b) >= 15:
            d["state_chg_tick_lsb_10ms"] = get_le16(13)
    except Exception:
        # 部分しか取れない場合は空dict or 部分返却
        pass
    return d

def load_phone_csv(path: str) -> pd.DataFrame:
    """
    AndroidロガーCSVの標準化。
    期待列:
      recv_unix_ms, device_id, rssi_dbm, mfg_raw_hex, seq, tick_lsb_10ms, state, u_q_0_255, battery_pct
    フィールドが無い場合はmfg_raw_hexから復元を試みる。
    """
    df = pd.read_csv(path)
    # 標準化
    df.columns = [c.strip() for c in df.columns]
    # 時刻
    if "recv_unix_ms" in df.columns:
        df["recv_ts"] = pd.to_datetime(df["recv_unix_ms"], unit="ms", utc=True)
    elif "timestamp" in df.columns:
        df["recv_ts"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
    else:
        raise ValueError(f"recv_unix_ms/timestamp not found in {path}")
    # RSSI
    if "rssi_dbm" not in df.columns and "rssi" in df.columns:
        df["rssi_dbm"] = df["rssi"]
    # パース試行
    need_fields = ["seq","tick_lsb_10ms","state","u_q_0_255","battery_pct","flags","state_seq","state_chg_tick_lsb_10ms"]
    missing = [f for f in need_fields if f not in df.columns]
    if missing and "mfg_raw_hex" in df.columns:
        parsed = df["mfg_raw_hex"].astype(str).apply(parse_mfg_payload_hex)
        extra = pd.json_normalize(parsed)
        for k in ["seq","tick_lsb_10ms","state","u_q","battery_pct","flags","state_seq","state_chg_tick_lsb_10ms"]:
            if k in extra.columns:
                name = "u_q_0_255" if k == "u_q" else k
                df[name] = df.get(name, extra[k])
    # データ型・欠損処理
    for c in ["seq","tick_lsb_10ms","state","u_q_0_255","battery_pct","flags","state_seq","state_chg_tick_lsb_10ms"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    # device_id
    if "device_id" not in df.columns:
        df["device_id"] = "dev0"
    df = df.sort_values("recv_ts").reset_index(drop=True)
    return df

def load_uart_log(path: str) -> pd.DataFrame:
    """
    UARTログ（任意）の軽量パーサ。
    期待: "RUN_START", "RUN_END", "STATE", "CFG", "ADV"などのキーワードを含む行
    出力: ts(推定), level, msg, kv（辞書）
    """
    rows = []
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                s = line.strip()
                if not s:
                    continue
                # 簡易: 先頭に時刻があれば拾う
                ts = None
                # キー=バリューを抽出
                kv = {}
                parts = s.split()
                for p in parts:
                    if "=" in p:
                        k,v = p.split("=",1)
                        kv[k] = v
                rows.append({"raw": s, "ts": ts, "kv": kv})
        df = pd.DataFrame(rows)
    except FileNotFoundError:
        df = pd.DataFrame(columns=["raw","ts","kv"])
    return df

def load_meta_json(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)
```

セル4: 時系列整形（tickアンラップ、オフセット推定、seqアンラップ、欠落推定）
```
TICK_UNIT_MS = 10.0
TICK_MOD = 65536  # 16bit

def unwrap_series_mod(vals: pd.Series, mod: int) -> np.ndarray:
    """有限幅（mod）で循環する正整数列をアンラップ"""
    arr = vals.fillna(method="ffill").fillna(0).astype(int).values
    out = np.zeros_like(arr, dtype=np.int64)
    prev = arr[0]
    k = 0
    out[0] = arr[0]
    for i in range(1, len(arr)):
        cur = arr[i]
        # ラップ検知：大きな負差分
        if cur + k*mod < prev:
            k += 1
        out[i] = cur + k*mod
        prev = cur + k*mod
    return out

def estimate_offset_with_sync(df_phone: pd.DataFrame) -> float:
    """
    SYNCフェーズ（flags bit0=1想定）の最初の広告からオフセットを推定。
    返り値: phone_unix_ms - dev_tick_ms （ms）
    フラグが無い場合は先頭10件で近似。
    """
    d = df_phone.copy()
    if "flags" in d.columns and d["flags"].notna().any():
        sync = d[(d["flags"].fillna(0).astype(int) & 0x1) == 1]
    else:
        sync = d.head(10)
    if "tick_lsb_10ms" not in d.columns or sync.empty:
        return np.nan
    ticks_unwrapped = unwrap_series_mod(sync["tick_lsb_10ms"], TICK_MOD)
    dev_ms0 = ticks_unwrapped[0] * TICK_UNIT_MS
    phone_ms0 = sync["recv_ts"].iloc[0].value / 1e6  # ns→ms
    return phone_ms0 - dev_ms0

def enrich_phone_with_time_axes(df_phone: pd.DataFrame) -> pd.DataFrame:
    """
    phoneログに dev_tick_ms_unwrapped, est_dev_time_ms, seq_unwrapped, inter-arrival 等を付与
    """
    d = df_phone.copy()
    if "tick_lsb_10ms" in d.columns and d["tick_lsb_10ms"].notna().any():
        d["tick_unwrapped"] = unwrap_series_mod(d["tick_lsb_10ms"].fillna(0).astype(int), TICK_MOD)
        d["dev_tick_ms"] = d["tick_unwrapped"] * TICK_UNIT_MS
        offset = estimate_offset_with_sync(d)
        d["offset_ms_est"] = offset
        if not np.isnan(offset):
            d["est_dev_time_ms"] = d["dev_tick_ms"] + offset
            d["est_dev_time"] = pd.to_datetime(d["est_dev_time_ms"], unit="ms", utc=True)
        else:
            d["est_dev_time"] = pd.NaT
    else:
        d["est_dev_time"] = pd.NaT
    # seqのアンラップ→欠落率算出に利用
    if "seq" in d.columns and d["seq"].notna().any():
        d["seq_unwrapped"] = unwrap_series_mod(d["seq"].fillna(0).astype(int), 256)
        # 欠落数 = 連続差分-1の正部分
        diffs = d["seq_unwrapped"].diff().fillna(0)
        d["miss_count_since_prev"] = (diffs - 1).clip(lower=0).astype(int)
    else:
        d["seq_unwrapped"] = np.arange(len(d))
        d["miss_count_since_prev"] = 0
    # inter-arrival（受信間隔）
    d["recv_dt_ms"] = d["recv_ts"].diff().dt.total_seconds().mul(1000)
    return d
```

セル5: 一括ローディング（runディレクトリを走査して集約）
```
def discover_runs(data_root=DATA_ROOT):
    """
    data/raw/YYYYMMDD/subject_id/condition/ 以下にある run を自動検出
    各runは ppk2_*.csv, phone_*.csv, uart_*.log, meta_*.json を含む想定
    """
    runs = []
    for meta_path in glob.glob(os.path.join(data_root, "*", "*", "*", "meta_*.json")):
        base = os.path.dirname(meta_path)
        date = base.split(os.sep)[-3]
        subject = base.split(os.sep)[-2]
        condition = base.split(os.sep)[-1]
        condition_disp = CONDITION_MAP.get(condition, condition)
        run_id = os.path.splitext(os.path.basename(meta_path))[0].replace("meta_","")
        ppk2 = glob.glob(os.path.join(base, f"ppk2_{run_id}.csv"))
        phone = glob.glob(os.path.join(base, f"phone_{run_id}.csv"))
        uart = glob.glob(os.path.join(base, f"uart_{run_id}.log"))
        runs.append({
            "date": date, "subject": subject, "condition": condition_disp, "run_id": run_id,
            "base": base,
            "ppk2": ppk2[0] if ppk2 else None,
            "phone": phone[0] if phone else None,
            "uart": uart[0] if uart else None,
            "meta": meta_path
        })
    return pd.DataFrame(runs)

runs_df = discover_runs()
print("Detected runs:", len(runs_df))
runs_df.head()
```

セル6: 指標算出のコア関数（1 run）
```
def compute_metrics_for_run(run: dict) -> dict:
    """
    1つのrunから主要指標を算出
    出力: dict（表形式にそのまま入る）
    """
    meta = load_meta_json(run["meta"])
    cond = run["condition"]
    subject = run["subject"]
    run_id = run["run_id"]
    # PPK2
    if run["ppk2"] is None or not os.path.exists(run["ppk2"]):
        raise FileNotFoundError(f"PPK2 missing: {run}")
    ppk = load_ppk2_csv(run["ppk2"])
    I_avg_uA = ppk["i_mA"].mean() * 1000.0
    V_mean = ppk["v_V"].mean()
    E_mJ_per_min = I_avg_uA/1e6 * V_mean * 60.0 * 1000.0  # mJ/min
    
    # Phone
    if run["phone"] is None or not os.path.exists(run["phone"]):
        raise FileNotFoundError(f"Phone CSV missing: {run}")
    ph = load_phone_csv(run["phone"])
    ph = enrich_phone_with_time_axes(ph)
    # 欠落率（seqベース）
    miss = ph["miss_count_since_prev"].sum()
    recv = len(ph)
    expect = recv + miss
    loss_rate_pct = (miss / expect * 100.0) if expect > 0 else np.nan
    # RSSI
    rssi_med = ph["rssi_dbm"].median() if "rssi_dbm" in ph.columns else np.nan
    # 受信間隔統計
    inter_ms = ph["recv_dt_ms"].dropna()
    adv_p50_ms = inter_ms.median() if not inter_ms.empty else np.nan
    adv_p95_ms = inter_ms.quantile(0.95) if not inter_ms.empty else np.nan
    
    # 遅延推定（state_change_tickがある場合に厳密、無い場合は1広告周期下限で近似）
    p50_delay_ms = np.nan
    p95_delay_ms = np.nan
    if "state_chg_tick_lsb_10ms" in ph.columns and ph["state_chg_tick_lsb_10ms"].notna().any():
        # state_changeはflags bit1でマーキングされている前提
        changes = ph[(ph.get("flags", 0).fillna(0).astype(int) & 0x2) == 0x2]
        if not changes.empty and not ph["offset_ms_est"].isna().all():
            # 各changeについて、受信時刻 - 推定デバイス時刻 で遅延を測る
            off = ph["offset_ms_est"].dropna().iloc[0]
            dev_ms = unwrap_series_mod(changes["state_chg_tick_lsb_10ms"].astype(int), TICK_MOD) * TICK_UNIT_MS
            rx_ms = changes["recv_ts"].astype("int64")/1e6
            delays = rx_ms - (dev_ms + off)
            p50_delay_ms = np.median(delays)
            p95_delay_ms = np.quantile(delays, 0.95)
    else:
        # 近似：p50/p95の受信間隔で代替（遅延下限）
        p50_delay_ms = adv_p50_ms
        p95_delay_ms = adv_p95_ms
    
    # HAR精度（粗カテゴリ）：タスクスケジュールから区間ラベル付与
    # meta に schedule: [{"start_s":..,"end_s":..,"label":"Quiet/Active"}] がある想定
    F1_macro = np.nan
    if "schedule" in meta:
        # 推定時刻軸は phone 受信時刻（または est_dev_time）を使用
        ts = ph["recv_ts"]
        # 区間ラベル関数
        sched = meta["schedule"]
        # scheduleはrunの相対秒。run開始時刻を推定（先頭受信時刻を0とみなす）
        t0 = ts.iloc[0]
        rel_s = (ts - t0).dt.total_seconds()
        gt = []
        for t in rel_s:
            lab = None
            for seg in sched:
                if seg["start_s"] <= t <= seg["end_s"]:
                    lab = seg["label"]
                    break
            gt.append(lab if lab is not None else "Unknown")
        gt = pd.Series(gt)
        # 予測はphoneのstate（0/1/2）をQuiet/Activeにマッピング
        if "state" in ph.columns and ph["state"].notna().any():
            pred = ph["state"].astype(float).copy()
            # 2(不確実)の扱い：u_q>=128ならActive、それ以外Quiet
            if "u_q_0_255" in ph.columns:
                pred = pred.where(pred != 2, (ph["u_q_0_255"] >= 128).astype(int))
            else:
                pred = pred.where(pred != 2, 0)  # デフォルトQuiet寄せ
            pred = pred.map({0:"Quiet", 1:"Active"})
            # Unknownを除外
            mask = gt.isin(["Quiet","Active"]) & pred.isin(["Quiet","Active"])
            if mask.any():
                F1_macro = f1_score(gt[mask], pred[mask], average="macro")
    
    out = dict(
        run_id=run_id, subject=subject, condition=cond,
        I_avg_uA=I_avg_uA, V_mean=V_mean, E_mJ_per_min=E_mJ_per_min,
        loss_rate_pct=loss_rate_pct, rssi_med=rssi_med,
        inter_p50_ms=adv_p50_ms, inter_p95_ms=adv_p95_ms,
        delay_p50_ms=p50_delay_ms, delay_p95_ms=p95_delay_ms,
        F1_macro=F1_macro,
        recv_count=recv, miss_count=miss, expect_count=expect
    )
    return out, ppk, ph, meta
```

セル7: 全runの計算とサマリ表の作成・保存
```
results = []
run_artifacts = {}  # run_id -> (ppk, ph, meta)
for _, row in runs_df.iterrows():
    try:
        res, ppk, ph, meta = compute_metrics_for_run(row.to_dict())
        results.append(res)
        run_artifacts[res["run_id"]] = (ppk, ph, meta, row.to_dict())
    except Exception as e:
        print("Error:", row["base"], e)

res_df = pd.DataFrame(results)
if not res_df.empty:
    res_df["subject"] = res_df["subject"].astype(str)
    res_df["condition"] = res_df["condition"].astype(str)
res_df.to_csv(os.path.join(OUT_ROOT, "summary_by_run.csv"), index=False)
res_df
```

セル8: 品質チェック（閾値、欠測、時間軸）
```
def qc_report(res_df: pd.DataFrame):
    if res_df.empty:
        print("No results.")
        return
    print("Runs:", len(res_df))
    print("Subjects:", sorted(res_df["subject"].unique()))
    print("Conditions:", sorted(res_df["condition"].unique()))
    print("\nLoss rate (pct) by condition:")
    print(res_df.groupby("condition")["loss_rate_pct"].describe()[["mean","50%","max"]])
    print("\nDelay p95 by condition:")
    print(res_df.groupby("condition")["delay_p95_ms"].describe()[["mean","50%","max"]])

qc_report(res_df)
```

セル9: 図表テンプレ（個別runの電流波形＋区間注釈）
```
def plot_ppk_current_with_annotations(run_id: str, save=True):
    if run_id not in run_artifacts:
        print("run not found:", run_id)
        return
    ppk, ph, meta, row = run_artifacts[run_id]
    fig, ax = plt.subplots(figsize=(12,4))
    ax.plot(ppk["t_s"], ppk["i_mA"], lw=0.6, color="#1f77b4")
    ax.set_xlabel("Time [s]")
    ax.set_ylabel("Current [mA]")
    ax.set_title(f"Current trace: {row['subject']} / {row['condition']} / {run_id}")
    # 区間注釈（Quiet/Active）
    if "schedule" in meta:
        for seg in meta["schedule"]:
            c = {"Quiet":"#a1d99b", "Active":"#fdae6b"}.get(seg["label"], "#cccccc")
            ax.axvspan(seg["start_s"], seg["end_s"], color=c, alpha=0.2)
            ax.text((seg["start_s"]+seg["end_s"])/2, ax.get_ylim()[1]*0.9, seg["label"],
                    ha="center", va="top", fontsize=9, color="#555")
    sns.despine()
    if save:
        path = os.path.join(FIG_ROOT, f"ppk_current_{run_id}.png")
        plt.savefig(path, bbox_inches="tight")
    plt.show()

# 任意のrun_idを指定（先頭を例に）
if not res_df.empty:
    plot_ppk_current_with_annotations(res_df["run_id"].iloc[0])
```

セル10: 図表テンプレ（受信間隔ヒスト/CDF、遅延CDF）
```
def plot_interarrival_and_delay(run_id: str, save=True):
    if run_id not in run_artifacts:
        print("run not found:", run_id); return
    ppk, ph, meta, row = run_artifacts[run_id]
    fig, axes = plt.subplots(1,2, figsize=(12,4))
    inter = ph["recv_dt_ms"].dropna()
    axes[0].hist(inter, bins=50, color="#2ca02c", alpha=0.8)
    axes[0].set_xlabel("Inter-arrival [ms]")
    axes[0].set_ylabel("Count")
    axes[0].set_title(f"Inter-arrival: {row['condition']}")
    # CDF
    x = np.sort(inter.values)
    y = np.arange(1, len(x)+1)/len(x) if len(x)>0 else np.array([])
    axes[1].plot(x, y, color="#9467bd")
    axes[1].set_xlabel("Inter-arrival [ms]")
    axes[1].set_ylabel("CDF")
    axes[1].set_title("Inter-arrival CDF")
    sns.despine()
    if save:
        path = os.path.join(FIG_ROOT, f"interarrival_{run_id}.png")
        plt.savefig(path, bbox_inches="tight")
    plt.show()

if not res_df.empty:
    plot_interarrival_and_delay(res_df["run_id"].iloc[0])
```

セル11: 図表テンプレ（Pareto散布：Energy–Latency–F1）
```
def plot_pareto_scatter(res_df: pd.DataFrame, save=True):
    if res_df.empty:
        return
    df = res_df.copy()
    # 軸: Energy (mJ/min) vs Delay p95 (ms), 色=condition, サイズ=F1
    plt.figure(figsize=(8,6))
    sc = sns.scatterplot(
        data=df, x="E_mJ_per_min", y="delay_p95_ms",
        hue="condition", size="F1_macro", sizes=(50,200), alpha=0.85
    )
    plt.xlabel("Energy [mJ/min]")
    plt.ylabel("Delay p95 [ms]")
    plt.title("Energy–Latency–F1 Pareto")
    plt.legend(bbox_to_anchor=(1.02, 1), loc="upper left")
    sns.despine()
    if save:
        path = os.path.join(FIG_ROOT, "pareto_energy_delay_f1.png")
        plt.savefig(path, bbox_inches="tight")
    plt.show()

plot_pareto_scatter(res_df)
```

セル12: 条件別サマリ表（平均±SD、箱ひげ）
```
def condition_summary(res_df: pd.DataFrame) -> pd.DataFrame:
    keys = ["I_avg_uA","E_mJ_per_min","delay_p95_ms","loss_rate_pct","F1_macro"]
    def fmt(g):
        return pd.Series({
            "n": len(g),
            **{k+"_mean": np.nanmean(g[k]) for k in keys},
            **{k+"_std":  np.nanstd(g[k], ddof=1) for k in keys}
        })
    summ = res_df.groupby("condition", as_index=False).apply(fmt)
    return summ

summ = condition_summary(res_df)
summ.to_csv(os.path.join(OUT_ROOT, "summary_by_condition.csv"), index=False)
summ
```

セル13: 統計比較（対応ありt検定・効果量）とバー図
```
def paired_compare(res_df: pd.DataFrame, subject_key="subject",
                   base=PAIR_BASE, prop=PAIR_PROP,
                   metric="E_mJ_per_min"):
    # 被験者内で base と prop をペアリング
    df = res_df[[subject_key,"condition",metric]].dropna()
    base_df = df[df["condition"]==base].set_index(subject_key)
    prop_df = df[df["condition"]==prop].set_index(subject_key)
    common = base_df.index.intersection(prop_df.index)
    x = prop_df.loc[common, metric].values
    y = base_df.loc[common, metric].values
    if len(common) < 2:
        return {"n": len(common), "t": np.nan, "p": np.nan, "d": np.nan}
    t, p = stats.ttest_rel(x, y, nan_policy="omit")
    # 効果量（Cohen's d for paired）: (x-y).mean() / sd(diff)
    diff = x - y
    d_eff = np.nanmean(diff) / (np.nanstd(diff, ddof=1) + 1e-12)
    return {"n": len(common), "t": t, "p": p, "d": d_eff}

for m in ["I_avg_uA","E_mJ_per_min","delay_p95_ms","loss_rate_pct"]:
    r = paired_compare(res_df, metric=m)
    print(f"{m}: n={r['n']}, t={r['t']:.3f}, p={r['p']:.4f}, d={r['d']:.3f}")

# バー図（平均±SD）
def bar_with_error(res_df: pd.DataFrame, metric="E_mJ_per_min", save=True):
    df = res_df.copy()
    plt.figure(figsize=(7,4))
    ax = sns.barplot(data=df, x="condition", y=metric, errorbar="sd", palette="Set2", capsize=.1)
    ax.set_xlabel("")
    ax.set_ylabel(metric)
    ax.set_title(metric + " by condition")
    plt.xticks(rotation=15)
    sns.despine()
    if save:
        plt.savefig(os.path.join(FIG_ROOT, f"bar_{metric}.png"), bbox_inches="tight")
    plt.show()

for m in ["I_avg_uA","E_mJ_per_min","delay_p95_ms","loss_rate_pct"]:
    bar_with_error(res_df, metric=m)
```

セル14: 被験者別・条件別の詳細表（論文の表1テンプレ）
```
def table_for_paper(res_df: pd.DataFrame) -> pd.DataFrame:
    cols = ["subject","condition","I_avg_uA","E_mJ_per_min","delay_p95_ms","loss_rate_pct","F1_macro","recv_count","miss_count"]
    df = res_df[cols].sort_values(["subject","condition"])
    # 四捨五入
    df["I_avg_uA"] = df["I_avg_uA"].round(1)
    df["E_mJ_per_min"] = df["E_mJ_per_min"].round(1)
    df["delay_p95_ms"] = df["delay_p95_ms"].round(1)
    df["loss_rate_pct"] = df["loss_rate_pct"].round(2)
    df["F1_macro"] = df["F1_macro"].round(3)
    df.to_csv(os.path.join(OUT_ROOT, "table_paper.csv"), index=False)
    return df

table_for_paper(res_df)
```

セル15: ランレベル診断図（RSSI vs 欠落、u_qの分布、状態遷移トレース）
```
def diag_run(run_id: str, save=True):
    if run_id not in run_artifacts: return
    ppk, ph, meta, row = run_artifacts[run_id]
    fig, axes = plt.subplots(1,3, figsize=(16,4))
    # RSSI vs inter-arrival
    axes[0].scatter(ph["rssi_dbm"], ph["recv_dt_ms"], s=10, alpha=0.6)
    axes[0].set_xlabel("RSSI [dBm]"); axes[0].set_ylabel("Inter-arrival [ms]")
    axes[0].set_title("RSSI vs Inter-arrival")
    # u_q 分布
    if "u_q_0_255" in ph.columns:
        sns.histplot(ph["u_q_0_255"], bins=30, kde=True, ax=axes[1], color="#ff7f0e")
        axes[1].set_xlabel("Uncertainty (0-255)"); axes[1].set_title("Uncertainty distribution")
    # 状態トレース
    if "state" in ph.columns:
        axes[2].plot(ph["recv_ts"], ph["state"], lw=0.7)
        axes[2].set_yticks([0,1,2]); axes[2].set_yticklabels(["Quiet","Active","Uncertain"])
        axes[2].set_title("State trace")
    plt.suptitle(f"Diagnostics: {row['subject']} / {row['condition']} / {run_id}")
    sns.despine()
    if save:
        plt.savefig(os.path.join(FIG_ROOT, f"diag_{run_id}.png"), bbox_inches="tight")
    plt.show()

if not res_df.empty:
    diag_run(res_df["run_id"].iloc[0])
```

セル16: レポート出力（Markdownのひな形生成）
```
def export_markdown_report(res_df: pd.DataFrame, out_path=os.path.join(OUT_ROOT, "report.md")):
    now = dt.datetime.now().strftime("%Y-%m-%d %H:%M")
    summ = condition_summary(res_df)
    lines = []
    lines.append(f"# Analysis Report ({now})")
    lines.append("")
    lines.append("## Summary by condition")
    lines.append("")
    lines.append(summ.to_markdown(index=False))
    lines.append("")
    lines.append("## Figures")
    lines.append("- pareto_energy_delay_f1.png")
    for p in glob.glob(os.path.join(FIG_ROOT, "ppk_current_*.png")):
        lines.append(f"- {os.path.basename(p)}")
    for p in glob.glob(os.path.join(FIG_ROOT, "bar_*.png")):
        lines.append(f"- {os.path.basename(p)}")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print("Report saved to", out_path)

export_markdown_report(res_df)
```

セル17: よくある差分への対応（フィールド欠損・カラム名違いの救済）
```
# 例: phone CSVに state/u_q がない場合、mfg_raw_hex からの復元を強制
# すでにload_phone_csv内で試行しているが、ここで追加の検査を実施
def audit_phone_fields(run_id: str):
    if run_id not in run_artifacts: return
    ppk, ph, meta, row = run_artifacts[run_id]
    missing = [c for c in ["seq","tick_lsb_10ms","state","u_q_0_255","battery_pct"] if c not in ph.columns]
    print("Missing fields:", missing)
    if "mfg_raw_hex" in ph.columns and missing:
        parsed = ph["mfg_raw_hex"].astype(str).apply(parse_mfg_payload_hex)
        extra = pd.json_normalize(parsed)
        print("Recoverable keys:", sorted([c for c in extra.columns if c in ["seq","tick_lsb_10ms","state","u_q","battery_pct"]]))
    else:
        print("mfg_raw_hex not available or no missing fields.")

if not res_df.empty:
    audit_phone_fields(res_df["run_id"].iloc[0])
```

セル18: 保存物の最終確認
```
print("Saved files:")
for p in sorted(glob.glob(os.path.join(OUT_ROOT, "*"))):
    print(" -", os.path.basename(p))
for p in sorted(glob.glob(os.path.join(FIG_ROOT, "*"))):
    print(" -", os.path.basename(p))
```

使い方メモ
- data/raw/以下の実データとmeta JSONを用意したら、上から順にセルを実行。
- 主要成果物は data/results/summary_by_run.csv, summary_by_condition.csv, table_paper.csv と figs/配下の画像群。
- 統計比較は Fixed-100ms vs Adaptive で行っています。条件名を変える場合はセル2の CONDITION_MAP と PAIR_* を調整してください。

オプション（必要なら追加）
- ブートストラップ信頼区間: 指標の95%CIを推定する関数を追加可能。
- iOS参考測定向けのフィルタ: 受信数が極端に少ないrunを除外するマスクを追加。
- 追加図: RSSI時系列＋状態遷移の重ね合わせ、欠落発生タイムラインなど。